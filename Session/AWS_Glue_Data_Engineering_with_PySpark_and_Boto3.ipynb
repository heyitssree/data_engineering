{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e19255",
   "metadata": {},
   "source": [
    "# AWS Glue Code Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71347cfb",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we will explore a provided piece of AWS Glue code that performs an ETL operation, joining sales data with customer data, and then writing the resulting dataset to an S3 bucket. We will go through the code step by step to understand its components and execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788fe254",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "salesDF = glueContext.create_dynamic_frame.from_catalog(\n",
    "             database=\"dojodatabase\",\n",
    "             table_name=\"sales\")\n",
    "customerDF = glueContext.create_dynamic_frame.from_catalog(\n",
    "             database=\"dojodatabase\",\n",
    "             table_name=\"customers\")\n",
    "\n",
    "customersalesDF = Join.apply(salesDF, customerDF, 'customerid', 'customerid')\n",
    "customersalesDF = customersalesDF.drop_fields(['customerid'])\n",
    "\n",
    "glueContext.write_dynamic_frame.from_options(customersalesDF, connection_type = \"s3\", connection_options = {\"path\": \"s3://dojo-data-lake/data/customer-sales\"}, format = \"json\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12831f",
   "metadata": {},
   "source": [
    "\n",
    "## Code Explanation\n",
    "\n",
    "### Importing Libraries and Creating Glue Context\n",
    "\n",
    "```python\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "```\n",
    "\n",
    "These lines import the necessary libraries and modules. The `awsglue.transforms` module provides classes for various transformations that can be performed on the data. The `awsglue.context` module is used to create a Glue context, which is needed to create DynamicFrames.\n",
    "\n",
    "```python\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "```\n",
    "\n",
    "Here, a Glue context is created from a Spark context. The Spark context is the entry point to any Spark functionality, and the Glue context allows AWS Glue to use Spark.\n",
    "\n",
    "### Reading Data from AWS Glue Catalog\n",
    "\n",
    "```python\n",
    "salesDF = glueContext.create_dynamic_frame.from_catalog(\n",
    "             database=\"dojodatabase\",\n",
    "             table_name=\"sales\")\n",
    "customerDF = glueContext.create_dynamic_frame.from_catalog(\n",
    "             database=\"dojodatabase\",\n",
    "             table_name=\"customers\")\n",
    "```\n",
    "\n",
    "These lines read data from tables in the AWS Glue Catalog into DynamicFrames. The Glue Catalog contains metadata about data stored in various locations, and it's a centralized repository for storing this metadata.\n",
    "\n",
    "### Joining the Sales and Customer Data\n",
    "\n",
    "```python\n",
    "customersalesDF = Join.apply(salesDF, customerDF, 'customerid', 'customerid')\n",
    "```\n",
    "\n",
    "This line performs a join operation between the sales data and customer data on the `customerid` field. The `Join.apply` method is used to join two DynamicFrames based on the specified keys.\n",
    "\n",
    "### Dropping Unnecessary Fields\n",
    "\n",
    "```python\n",
    "customersalesDF = customersalesDF.drop_fields(['customerid'])\n",
    "```\n",
    "\n",
    "This line drops the duplicate `customerid` field from the joined DynamicFrame. This is a common operation after a join to remove redundant columns.\n",
    "\n",
    "### Writing the Resulting Data to S3\n",
    "\n",
    "```python\n",
    "glueContext.write_dynamic_frame.from_options(customersalesDF, connection_type = \"s3\", connection_options = {\"path\": \"s3://dojo-data-lake/data/customer-sales\"}, format = \"json\")\n",
    "```\n",
    "\n",
    "This line writes the resulting joined and cleaned data to an S3 bucket in JSON format. The `write_dynamic_frame.from_options` method is used to write the DynamicFrame to a specified location, format, and connection type.\n",
    "\n",
    "### Summary\n",
    "\n",
    "This AWS Glue code snippet is a straightforward example of reading, transforming, and loading data using AWS Glue, PySpark, and boto3. It reads data from the AWS Glue Catalog, performs a join operation, cleans up the resulting data, and then writes it to an S3 bucket.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
